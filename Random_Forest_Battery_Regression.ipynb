{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "version_major": 2,
        "version_minor": 0,
        "state": {}
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amkayhani/FAIDM/blob/main/Random_Forest_Battery_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "616ee196",
      "metadata": {
        "id": "616ee196"
      },
      "source": [
        "# **Regression: Random Forests for Battery Thickness Prediction**\n",
        "\n",
        "## **Module Context**\n",
        "\n",
        "This notebook is part of the Regression: Random Forests teaching module delivered at WMG, University of Warwick (January 2026).\n",
        "\n",
        "## **Overview**\n",
        "\n",
        "The aim of this notebook is to develop a Random Forest regression model to predict the Equivalent Full Cycles (EFCs) at a specified State of Health threshold (SOH = 0.9) for lithium-ion battery cells.\n",
        "\n",
        "The model leverages charge–discharge profile characteristics extracted from early-life cycling data to estimate the cycle life of cells operating under diverse conditions. These include both laboratory-controlled test protocols and real-world electric vehicle (EV) usage profiles, highlighting the robustness of ensemble regression methods in practical energy-system applications.\n",
        "\n",
        "## **Learning Objectives**\n",
        "\n",
        "By the end of this notebook, students will be able to:\n",
        "- understand the use of Random Forests for regression in battery health prediction,\n",
        "- train and configure a Random Forest regressor,\n",
        "- evaluate regression performance using appropriate metrics, and\n",
        "- interpret model behaviour across different operating conditions.\n",
        "\n",
        "## **Notebook Scope**\n",
        "\n",
        "This notebook presents a complete and reproducible regression pipeline, including:\n",
        "- feature extraction from charge–discharge profiles,\n",
        "- definition of input variables and target EFC values,\n",
        "- Random Forest model training and hyperparameter selection,\n",
        "- performance evaluation and result interpretation.\n",
        "\n",
        "## **Module Delivery**\n",
        "\n",
        "Dr **Mona Faraji Niri** — Associate Professor of Energy Systems\n",
        "\n",
        "Dr **Hamidreza Farhadi Tolie** — Research Fellow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f47c22",
      "metadata": {
        "id": "61f47c22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from scipy.stats import skew, kurtosis\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0685d199",
      "metadata": {
        "id": "0685d199"
      },
      "source": [
        "## 1. Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f99c3672",
      "metadata": {
        "id": "f99c3672"
      },
      "outputs": [],
      "source": [
        "metadata_url = \"https://drive.google.com/uc?id=10plwp_22mc2E5MnKhUSi1ZJBviWb-7E6\"\n",
        "features_url = \"https://drive.google.com/uc?id=1_s0oQFlBhiSHRWs60iVX2RH5TulM533t\"\n",
        "target_url   = \"https://drive.google.com/uc?id=15q2jV1B6no7a16tafTYt779z5GPM98FU\"\n",
        "\n",
        "# Load CSVs into DataFrames\n",
        "df_features = pd.read_csv(features_url, index_col=0)\n",
        "df_soh90    = pd.read_csv(target_url, index_col=0)\n",
        "\n",
        "# Preview\n",
        "df_features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88d0d53e",
      "metadata": {
        "id": "88d0d53e"
      },
      "source": [
        "## 2. Train–Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc1825ac",
      "metadata": {
        "id": "dc1825ac"
      },
      "outputs": [],
      "source": [
        "X = df_features.values\n",
        "y = np.array(df_soh90['EFCs (with Diagnostic)'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9412a671",
      "metadata": {
        "id": "9412a671"
      },
      "source": [
        "## 3. Random Forest Model\n",
        "**Hyper parameters of the model:**\n",
        "- **n_estimators (default=100)** — Number of trees. More trees improve stability but increase training time. Example: 100, 500, 1000.\n",
        "- **max_depth (default=None)** — Maximum depth of trees. Use None for fully grown trees, or smaller values to reduce overfitting. Example: None, 5, 10.\n",
        "- **min_samples_split (default=2)** — Minimum samples to split a node. Larger values → more conservative trees. Example: 2, 5, 10.\n",
        "- **min_samples_leaf (default=1)** — Minimum samples at a leaf. Larger values smooth predictions. Example: 1, 2, 4.\n",
        "- **max_features (default=\"auto\")** — Features considered at each split. Options: \"sqrt\", \"log2\", or integer number of features.\n",
        "- **random_state** — Ensures reproducible results.\n",
        "- **n_jobs** — CPU cores to use. -1 = all cores.\n",
        "\n",
        "## Hyperparameter Optimisation with Random Forests and Validation Metrics\n",
        "\n",
        "In this step, we perform hyperparameter optimisation for the Random Forest regressor using **Optuna**, while evaluating model performance on a validation set (X_test, y_test).\n",
        "\n",
        "The goal is to find the combination of hyperparameters that maximises predictive performance (here measured by R²) for battery lifetime prediction, while also monitoring MAE and RMSE on the validation set.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. Optuna iteratively suggests hyperparameter combinations.\n",
        "2. For each trial, the Random Forest is trained on X_train, y_train.\n",
        "3. Predictions are made on X_test, and MAE, RMSE, and R² are calculated.\n",
        "4. The progress bar shows optimisation progress, and trial results are printed in real-time.\n",
        "5. At the end, the best parameter set is selected based on highest R², and final metrics are reported."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "jaLRSqZ9qIaX"
      },
      "id": "jaLRSqZ9qIaX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Objective function for Optuna\n",
        "def objective(trial):\n",
        "    rf = RandomForestRegressor(\n",
        "        n_estimators=trial.suggest_int('n_estimators', 100, 500),\n",
        "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
        "        min_samples_split=trial.suggest_int('min_samples_split', 2, 10),\n",
        "        min_samples_leaf=trial.suggest_int('min_samples_leaf', 1, 4),\n",
        "        max_features=trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"Trial {trial.number:03d} | R²: {r2:.3f} | MAE: {mae:.3f} | RMSE: {rmse:.3f} | Params: {trial.params}\")\n",
        "    return r2\n",
        "\n",
        "study = optuna.create_study(direction='maximize', study_name=\"RF_Validation_Optimisation\")\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "best_rf = RandomForestRegressor(**study.best_params, random_state=42, n_jobs=-1)\n",
        "best_rf.fit(X_train, y_train)\n",
        "y_pred_best = best_rf.predict(X_test)\n",
        "\n",
        "best_mae = mean_absolute_error(y_test, y_pred_best)\n",
        "best_rmse = np.sqrt(mean_squared_error(y_test, y_pred_best))\n",
        "best_r2 = r2_score(y_test, y_pred_best)\n",
        "\n",
        "print(\"\\n✅ Best Parameters Found:\")\n",
        "print(study.best_params)\n",
        "print(f\"MAE: {best_mae:.3f}, RMSE: {best_rmse:.3f}, R²: {best_r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "qMWmwWTYpnN-"
      },
      "id": "qMWmwWTYpnN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16ded89",
      "metadata": {
        "id": "b16ded89"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestRegressor(\n",
        "    n_estimators=354,\n",
        "    max_depth=9,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=\"sqrt\",\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aa555c4",
      "metadata": {
        "id": "9aa555c4"
      },
      "source": [
        "## 4. Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ebd76b9",
      "metadata": {
        "id": "6ebd76b9"
      },
      "outputs": [],
      "source": [
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, rmse, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23bbb6d",
      "metadata": {
        "id": "e23bbb6d"
      },
      "source": [
        "## 5. Visualisations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d48c3cba",
      "metadata": {
        "id": "d48c3cba"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.plot([y_test.min(), y_test.max()],\n",
        "         [y_test.min(), y_test.max()])\n",
        "plt.xlabel(\"Computed EFC\")\n",
        "plt.ylabel(\"Predicted EFC\")\n",
        "plt.title(\"Predictions vs Reality\")\n",
        "plt.show()"
      ]
    }
  ]
}
